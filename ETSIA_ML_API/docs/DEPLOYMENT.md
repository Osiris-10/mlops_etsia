# üöÄ Guide de D√©ploiement

Guide complet pour d√©ployer l'API de d√©tection de d√©pression en production.

---

## üìã Pr√©requis

- Python 3.8+
- Cl√© API LLM (OpenAI, Anthropic, ou Ollama install√©)
- Serveur Linux (Ubuntu 20.04+ recommand√©)
- Domaine avec SSL (recommand√© pour production)

---

## üè† D√©ploiement Local

### 1. Installation

```bash
# Cloner le projet
git clone <votre-repo>
cd ETSIA_ML_API

# Cr√©er environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Installer d√©pendances
pip install -r requirements.txt
```

### 2. Configuration

```bash
# Copier le template
cp .env.example .env

# √âditer avec vos cl√©s
nano .env
```

Exemple `.env` :
```env
LLM_PROVIDER=gpt
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini
LOG_LEVEL=INFO
```

### 3. Lancer l'API

```bash
# Mode d√©veloppement
uvicorn app.main:app --reload --port 8000

# Mode production
uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 4
```

### 4. Tester

```bash
# Health check
curl http://localhost:8000/health

# Pr√©diction
curl -X POST http://localhost:8000/api/v1/predict \
  -H "Content-Type: application/json" \
  -d '{"text": "I feel so sad"}'
```

---

## üê≥ D√©ploiement Docker

### 1. Cr√©er le Dockerfile

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Copier requirements
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copier le code
COPY app/ ./app/
COPY .env .env

# Exposer le port
EXPOSE 8000

# Lancer l'API
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 2. Build et Run

```bash
# Build
docker build -t depression-api .

# Run
docker run -p 8000:8000 --env-file .env depression-api

# Avec docker-compose
docker-compose up -d
```

### 3. docker-compose.yml

```yaml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    env_file:
      - .env
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

---

## ‚òÅÔ∏è D√©ploiement Cloud

### Option 1: Render

**Avantages:** Gratuit, simple, SSL automatique

**√âtapes:**

1. Cr√©er compte sur [render.com](https://render.com)

2. Cr√©er un nouveau Web Service

3. Connecter votre repo GitHub

4. Configuration :
   ```
   Build Command: pip install -r requirements.txt
   Start Command: uvicorn app.main:app --host 0.0.0.0 --port $PORT
   ```

5. Ajouter variables d'environnement :
   ```
   LLM_PROVIDER=gpt
   OPENAI_API_KEY=sk-...
   OPENAI_MODEL=gpt-4o-mini
   ```

6. D√©ployer !

**URL:** `https://votre-app.onrender.com`

---

### Option 2: Railway

**Avantages:** Simple, $5/mois, bon pour production

**√âtapes:**

1. Cr√©er compte sur [railway.app](https://railway.app)

2. Nouveau projet ‚Üí Deploy from GitHub

3. S√©lectionner votre repo

4. Railway d√©tecte automatiquement Python

5. Ajouter variables d'environnement

6. D√©ployer !

**URL:** `https://votre-app.railway.app`

---

### Option 3: Heroku

**Avantages:** Mature, scalable

**√âtapes:**

1. Installer Heroku CLI

2. Cr√©er Procfile :
   ```
   web: uvicorn app.main:app --host 0.0.0.0 --port $PORT
   ```

3. D√©ployer :
   ```bash
   heroku login
   heroku create votre-app
   git push heroku main
   heroku config:set OPENAI_API_KEY=sk-...
   ```

**URL:** `https://votre-app.herokuapp.com`

---

### Option 4: AWS (EC2)

**Avantages:** Contr√¥le total, scalable

**√âtapes:**

1. **Lancer une instance EC2**
   ```bash
   # Ubuntu 22.04 LTS
   # t2.micro (gratuit) ou t2.small
   ```

2. **Connecter via SSH**
   ```bash
   ssh -i votre-cle.pem ubuntu@votre-ip
   ```

3. **Installer d√©pendances**
   ```bash
   sudo apt update
   sudo apt install python3-pip python3-venv nginx -y
   ```

4. **Cloner et configurer**
   ```bash
   git clone <votre-repo>
   cd ETSIA_ML_API
   python3 -m venv venv
   source venv/bin/activate
   pip install -r requirements.txt
   ```

5. **Cr√©er service systemd**
   ```bash
   sudo nano /etc/systemd/system/depression-api.service
   ```

   ```ini
   [Unit]
   Description=Depression Detection API
   After=network.target

   [Service]
   User=ubuntu
   WorkingDirectory=/home/ubuntu/ETSIA_ML_API
   Environment="PATH=/home/ubuntu/ETSIA_ML_API/venv/bin"
   EnvironmentFile=/home/ubuntu/ETSIA_ML_API/.env
   ExecStart=/home/ubuntu/ETSIA_ML_API/venv/bin/uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 4
   Restart=always

   [Install]
   WantedBy=multi-user.target
   ```

6. **D√©marrer le service**
   ```bash
   sudo systemctl daemon-reload
   sudo systemctl start depression-api
   sudo systemctl enable depression-api
   sudo systemctl status depression-api
   ```

7. **Configurer Nginx**
   ```bash
   sudo nano /etc/nginx/sites-available/depression-api
   ```

   ```nginx
   server {
       listen 80;
       server_name votre-domaine.com;

       location / {
           proxy_pass http://127.0.0.1:8000;
           proxy_set_header Host $host;
           proxy_set_header X-Real-IP $remote_addr;
           proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
           proxy_set_header X-Forwarded-Proto $scheme;
       }
   }
   ```

   ```bash
   sudo ln -s /etc/nginx/sites-available/depression-api /etc/nginx/sites-enabled/
   sudo nginx -t
   sudo systemctl restart nginx
   ```

8. **SSL avec Let's Encrypt**
   ```bash
   sudo apt install certbot python3-certbot-nginx -y
   sudo certbot --nginx -d votre-domaine.com
   ```

**URL:** `https://votre-domaine.com`

---

### Option 5: Google Cloud Run

**Avantages:** Serverless, pay-per-use, scalable

**√âtapes:**

1. **Installer gcloud CLI**

2. **Cr√©er Dockerfile** (voir section Docker)

3. **D√©ployer**
   ```bash
   gcloud auth login
   gcloud config set project votre-projet
   
   gcloud run deploy depression-api \
     --source . \
     --platform managed \
     --region us-central1 \
     --allow-unauthenticated \
     --set-env-vars OPENAI_API_KEY=sk-...
   ```

**URL:** `https://depression-api-xxx.run.app`

---

## üîí Configuration SSL/HTTPS

### Avec Nginx + Let's Encrypt

```bash
# Installer certbot
sudo apt install certbot python3-certbot-nginx

# Obtenir certificat
sudo certbot --nginx -d votre-domaine.com

# Renouvellement automatique
sudo certbot renew --dry-run
```

### Avec Cloudflare

1. Ajouter votre domaine √† Cloudflare
2. Activer SSL/TLS (Full)
3. Activer "Always Use HTTPS"
4. Configurer DNS vers votre serveur

---

## üìä Monitoring et Logs

### Logs avec systemd

```bash
# Voir les logs
sudo journalctl -u depression-api -f

# Logs des derni√®res 24h
sudo journalctl -u depression-api --since "24 hours ago"
```

### Monitoring avec Prometheus

```yaml
# prometheus.yml
scrape_configs:
  - job_name: 'depression-api'
    static_configs:
      - targets: ['localhost:8000']
```

### Alertes avec Sentry

```python
# app/main.py
import sentry_sdk

sentry_sdk.init(
    dsn="votre-dsn-sentry",
    traces_sample_rate=1.0
)
```

---

## üîß Optimisations Production

### 1. Workers Uvicorn

```bash
# Nombre de workers = (2 x CPU cores) + 1
uvicorn app.main:app --workers 4 --host 0.0.0.0 --port 8000
```

### 2. Gunicorn + Uvicorn

```bash
pip install gunicorn

gunicorn app.main:app \
  --workers 4 \
  --worker-class uvicorn.workers.UvicornWorker \
  --bind 0.0.0.0:8000
```

### 3. Cache Redis

```python
# app/services/cache.py
import redis

cache = redis.Redis(host='localhost', port=6379, db=0)

def get_cached_prediction(text):
    key = f"pred:{hash(text)}"
    cached = cache.get(key)
    if cached:
        return json.loads(cached)
    return None

def cache_prediction(text, result):
    key = f"pred:{hash(text)}"
    cache.setex(key, 3600, json.dumps(result))  # 1h TTL
```

### 4. Rate Limiting

```python
# app/main.py
from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter

@app.post("/api/v1/predict")
@limiter.limit("10/minute")
async def predict(request: Request, data: PredictRequest):
    ...
```

---

## üß™ Tests en Production

### Health Check

```bash
# Simple
curl https://votre-api.com/health

# Avec monitoring
while true; do
  curl -s https://votre-api.com/health | jq .status
  sleep 60
done
```

### Load Testing

```bash
# Installer Apache Bench
sudo apt install apache2-utils

# Test de charge
ab -n 1000 -c 10 -p data.json -T application/json \
  https://votre-api.com/api/v1/predict
```

### Smoke Tests

```python
import requests

def test_production():
    base_url = "https://votre-api.com"
    
    # Health check
    r = requests.get(f"{base_url}/health")
    assert r.status_code == 200
    
    # Pr√©diction
    r = requests.post(
        f"{base_url}/api/v1/predict",
        json={"text": "I feel sad"}
    )
    assert r.status_code == 200
    assert "prediction" in r.json()
```

---

## üîÑ CI/CD

### GitHub Actions

```yaml
# .github/workflows/deploy.yml
name: Deploy to Production

on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      
      - name: Deploy to Render
        env:
          RENDER_API_KEY: ${{ secrets.RENDER_API_KEY }}
        run: |
          curl -X POST https://api.render.com/deploy/...
```

---

## üìà Scaling

### Horizontal Scaling

```bash
# Avec Docker Swarm
docker swarm init
docker service create \
  --name depression-api \
  --replicas 3 \
  --publish 8000:8000 \
  depression-api:latest

# Avec Kubernetes
kubectl apply -f deployment.yaml
kubectl scale deployment depression-api --replicas=5
```

### Load Balancer

```nginx
# nginx.conf
upstream api_backend {
    server 10.0.1.1:8000;
    server 10.0.1.2:8000;
    server 10.0.1.3:8000;
}

server {
    location / {
        proxy_pass http://api_backend;
    }
}
```

---

## üí∞ Co√ªts Estim√©s

### Cloud Providers

| Provider | Configuration | Co√ªt/mois |
|----------|--------------|-----------|
| Render | Free tier | $0 |
| Railway | Hobby | $5 |
| Heroku | Hobby | $7 |
| AWS EC2 | t2.micro | $8 |
| Google Cloud Run | Pay-per-use | $5-20 |

### LLM API

| Provider | Co√ªt/1M tokens | Co√ªt/1000 requ√™tes |
|----------|----------------|---------------------|
| GPT-4o-mini | $0.15 input, $0.60 output | ~$0.60 |
| Claude Sonnet | $3 input, $15 output | ~$15 |
| Llama local | $0 | $0 |

**Recommandation:** Render (gratuit) + GPT-4o-mini (~$0.60/1000 requ√™tes)

---

## üÜò Troubleshooting

### Probl√®me : API ne d√©marre pas

```bash
# V√©rifier les logs
sudo journalctl -u depression-api -n 50

# V√©rifier le port
sudo netstat -tulpn | grep 8000

# Tester manuellement
source venv/bin/activate
uvicorn app.main:app --reload
```

### Probl√®me : Erreur LLM

```bash
# V√©rifier la cl√© API
echo $OPENAI_API_KEY

# Tester directement
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer $OPENAI_API_KEY"
```

### Probl√®me : Timeout

```python
# Augmenter le timeout
# app/services/llm_service.py
response = requests.post(..., timeout=60)
```

---

## üìû Support

Pour toute question sur le d√©ploiement :
- **Documentation:** Voir README.md
- **Issues:** GitHub Issues
- **Email:** √âquipe YANSNET

---

**Version:** 1.0.0  
**Derni√®re mise √† jour:** Janvier 2025
