# üìã Contrat d'API - D√©tection de D√©pression

Documentation compl√®te de l'API REST pour la d√©tection de d√©pression avec LLM.

---

## üåê Base URL

```
http://localhost:8000
```

En production, remplacer par votre domaine.

---

## üîê Authentification

Actuellement, l'API est ouverte. Pour ajouter une authentification :

```env
API_KEY=votre-cl√©-secr√®te
```

Puis inclure dans les headers :
```
X-API-Key: votre-cl√©-secr√®te
```

---

## üìç Endpoints

### 1. Health Check

V√©rifie l'√©tat de l'API.

**Endpoint:** `GET /health`

**Response:**
```json
{
  "status": "healthy",
  "version": "1.0.0",
  "llm_provider": "gpt",
  "llm_model": "gpt-4o-mini",
  "timestamp": "2025-01-16T10:30:00Z"
}
```

**Status Codes:**
- `200 OK` - API fonctionnelle

---

### 2. Pr√©diction Simple

Analyse un texte et d√©tecte les signes de d√©pression.

**Endpoint:** `POST /api/v1/predict`

**Headers:**
```
Content-Type: application/json
```

**Request Body:**
```json
{
  "text": "I feel so sad and hopeless, I don't want to live anymore",
  "include_reasoning": true
}
```

**Parameters:**

| Param√®tre | Type | Requis | Description |
|-----------|------|--------|-------------|
| `text` | string | ‚úÖ | Texte √† analyser (1-5000 caract√®res) |
| `include_reasoning` | boolean | ‚ùå | Inclure l'explication (d√©faut: true) |

**Response:**
```json
{
  "prediction": "D√âPRESSION",
  "confidence": 0.85,
  "severity": "√âlev√©e",
  "reasoning": "Le texte exprime un d√©sespoir profond et une tristesse intense, avec des pens√©es suicidaires explicites. Les marqueurs linguistiques indiquent une d√©tresse psychologique s√©v√®re.",
  "timestamp": "2025-01-16T10:30:00Z",
  "model_used": "gpt-4o-mini"
}
```

**Response Fields:**

| Champ | Type | Description |
|-------|------|-------------|
| `prediction` | enum | `"D√âPRESSION"` ou `"NORMAL"` |
| `confidence` | float | Niveau de confiance (0.0 - 1.0) |
| `severity` | enum | `"Aucune"`, `"Faible"`, `"Moyenne"`, `"√âlev√©e"`, `"Critique"` |
| `reasoning` | string | Explication du raisonnement (si demand√©) |
| `timestamp` | datetime | Timestamp ISO 8601 |
| `model_used` | string | Mod√®le LLM utilis√© |

**Status Codes:**
- `200 OK` - Pr√©diction r√©ussie
- `400 Bad Request` - Requ√™te invalide (texte vide, trop long, etc.)
- `500 Internal Server Error` - Erreur serveur

**Exemples de Requ√™tes:**

```bash
# Avec reasoning
curl -X POST http://localhost:8000/api/v1/predict \
  -H "Content-Type: application/json" \
  -d '{
    "text": "I feel so sad and hopeless",
    "include_reasoning": true
  }'

# Sans reasoning (plus rapide)
curl -X POST http://localhost:8000/api/v1/predict \
  -H "Content-Type: application/json" \
  -d '{
    "text": "I feel so sad and hopeless",
    "include_reasoning": false
  }'
```

**Exemples Python:**

```python
import requests

# Pr√©diction simple
response = requests.post(
    "http://localhost:8000/api/v1/predict",
    json={
        "text": "I feel so sad and hopeless",
        "include_reasoning": True
    }
)

result = response.json()
print(f"Pr√©diction: {result['prediction']}")
print(f"Confiance: {result['confidence']:.2%}")
print(f"S√©v√©rit√©: {result['severity']}")
print(f"Raisonnement: {result['reasoning']}")
```

---

### 3. Pr√©diction Batch

Analyse plusieurs textes en une seule requ√™te.

**Endpoint:** `POST /api/v1/batch-predict`

**Headers:**
```
Content-Type: application/json
```

**Request Body:**
```json
{
  "texts": [
    "I'm so happy today",
    "I feel worthless and empty",
    "Just finished a great workout"
  ],
  "include_reasoning": false
}
```

**Parameters:**

| Param√®tre | Type | Requis | Description |
|-----------|------|--------|-------------|
| `texts` | array[string] | ‚úÖ | Liste de textes (1-100 textes) |
| `include_reasoning` | boolean | ‚ùå | Inclure les explications (d√©faut: false) |

**Response:**
```json
{
  "results": [
    {
      "text": "I'm so happy today",
      "prediction": "NORMAL",
      "confidence": 0.95,
      "severity": "Aucune",
      "reasoning": null
    },
    {
      "text": "I feel worthless and empty",
      "prediction": "D√âPRESSION",
      "confidence": 0.88,
      "severity": "√âlev√©e",
      "reasoning": null
    },
    {
      "text": "Just finished a great workout",
      "prediction": "NORMAL",
      "confidence": 0.92,
      "severity": "Aucune",
      "reasoning": null
    }
  ],
  "total_processed": 3,
  "processing_time": 1.2,
  "model_used": "gpt-4o-mini"
}
```

**Response Fields:**

| Champ | Type | Description |
|-------|------|-------------|
| `results` | array | Liste des pr√©dictions |
| `total_processed` | integer | Nombre de textes trait√©s |
| `processing_time` | float | Temps de traitement (secondes) |
| `model_used` | string | Mod√®le LLM utilis√© |

**Status Codes:**
- `200 OK` - Pr√©dictions r√©ussies
- `400 Bad Request` - Requ√™te invalide (liste vide, trop de textes, etc.)
- `500 Internal Server Error` - Erreur serveur

**Exemples de Requ√™tes:**

```bash
curl -X POST http://localhost:8000/api/v1/batch-predict \
  -H "Content-Type: application/json" \
  -d '{
    "texts": [
      "I am so happy today",
      "I feel worthless and empty"
    ],
    "include_reasoning": false
  }'
```

**Exemples Python:**

```python
import requests

# Pr√©diction batch
texts = [
    "I'm so happy today",
    "I feel worthless and empty",
    "Just finished a great workout"
]

response = requests.post(
    "http://localhost:8000/api/v1/batch-predict",
    json={
        "texts": texts,
        "include_reasoning": False
    }
)

result = response.json()
print(f"Trait√© {result['total_processed']} textes en {result['processing_time']:.2f}s")

for item in result['results']:
    print(f"\nTexte: {item['text']}")
    print(f"Pr√©diction: {item['prediction']} ({item['confidence']:.2%})")
```

---

## üîÑ Codes de Statut HTTP

| Code | Signification | Description |
|------|---------------|-------------|
| 200 | OK | Requ√™te r√©ussie |
| 400 | Bad Request | Requ√™te invalide (validation √©chou√©e) |
| 422 | Unprocessable Entity | Donn√©es invalides (Pydantic validation) |
| 500 | Internal Server Error | Erreur serveur |
| 503 | Service Unavailable | Service LLM indisponible |

---

## ‚ùå Gestion des Erreurs

Toutes les erreurs retournent un format standard :

```json
{
  "error": "Erreur de pr√©diction",
  "detail": "Le service LLM est temporairement indisponible",
  "timestamp": "2025-01-16T10:30:00Z"
}
```

**Exemples d'Erreurs:**

### Texte vide
```json
{
  "detail": [
    {
      "loc": ["body", "text"],
      "msg": "Le texte ne peut pas √™tre vide",
      "type": "value_error"
    }
  ]
}
```

### Texte trop long
```json
{
  "detail": [
    {
      "loc": ["body", "text"],
      "msg": "ensure this value has at most 5000 characters",
      "type": "value_error.any_str.max_length"
    }
  ]
}
```

### Service LLM indisponible
```json
{
  "error": "Erreur de pr√©diction",
  "detail": "Erreur: Connection timeout",
  "timestamp": "2025-01-16T10:30:00Z"
}
```

---

## üìä Limites et Quotas

| Limite | Valeur | Description |
|--------|--------|-------------|
| Taille max texte | 5000 caract√®res | Par texte individuel |
| Batch max | 100 textes | Par requ√™te batch |
| Rate limit | Aucun | √Ä configurer selon vos besoins |
| Timeout | 30 secondes | Par requ√™te |

---

## üîß Configuration des Providers

### GPT (OpenAI)

```env
LLM_PROVIDER=gpt
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini
```

**Mod√®les disponibles:**
- `gpt-4o-mini` (recommand√©) - Rapide et √©conomique
- `gpt-4o` - Plus puissant mais plus cher
- `gpt-3.5-turbo` - √âconomique mais moins pr√©cis

### Claude (Anthropic)

```env
LLM_PROVIDER=claude
ANTHROPIC_API_KEY=sk-ant-...
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
```

**Mod√®les disponibles:**
- `claude-3-5-sonnet-20241022` (recommand√©) - Meilleur √©quilibre
- `claude-3-opus-20240229` - Plus puissant
- `claude-3-haiku-20240307` - Plus rapide

### Ollama (Local)

```env
LLM_PROVIDER=local
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2
```

**Mod√®les disponibles:**
- `llama3.2` (recommand√©) - Bon √©quilibre
- `llama3.1` - Plus puissant
- `mistral` - Alternative

---

## üìà Performances

### Latence Moyenne

| Provider | Latence | Co√ªt par requ√™te |
|----------|---------|------------------|
| GPT-4o-mini | ~300ms | ~$0.00006 |
| Claude Sonnet | ~300ms | ~$0.00015 |
| Llama local | ~300ms | Gratuit |

### Throughput

- **Single request:** ~3 req/s
- **Batch (10 textes):** ~0.3 batch/s
- **Batch (100 textes):** ~0.03 batch/s

---

## üß™ Tests

### Test avec curl

```bash
# Health check
curl http://localhost:8000/health

# Pr√©diction simple
curl -X POST http://localhost:8000/api/v1/predict \
  -H "Content-Type: application/json" \
  -d '{"text": "I feel so sad"}'

# Batch
curl -X POST http://localhost:8000/api/v1/batch-predict \
  -H "Content-Type: application/json" \
  -d '{"texts": ["Happy", "Sad"]}'
```

### Test avec Python

```python
import requests

BASE_URL = "http://localhost:8000"

# Health check
response = requests.get(f"{BASE_URL}/health")
print(response.json())

# Pr√©diction
response = requests.post(
    f"{BASE_URL}/api/v1/predict",
    json={"text": "I feel so sad and hopeless"}
)
print(response.json())
```

---

## üìö Documentation Interactive

L'API fournit une documentation interactive via Swagger UI :

- **Swagger UI:** http://localhost:8000/docs
- **ReDoc:** http://localhost:8000/redoc

Ces interfaces permettent de :
- Voir tous les endpoints
- Tester les requ√™tes directement
- Voir les sch√©mas de donn√©es
- T√©l√©charger le sch√©ma OpenAPI

---

## üîí S√©curit√©

### Recommandations

1. **HTTPS en production** - Toujours utiliser HTTPS
2. **API Key** - Ajouter une authentification par cl√©
3. **Rate Limiting** - Limiter le nombre de requ√™tes
4. **CORS** - Configurer les origines autoris√©es
5. **Validation** - Toutes les entr√©es sont valid√©es par Pydantic

### Exemple de Configuration S√©curis√©e

```env
# API Key
API_KEY=votre-cl√©-secr√®te-complexe

# CORS
CORS_ORIGINS=https://yourdomain.com,https://app.yourdomain.com

# HTTPS
# Utiliser un reverse proxy (nginx, traefik)
```

---

## üìû Support

Pour toute question sur l'API :
- **Documentation:** http://localhost:8000/docs
- **Issues:** GitHub Issues
- **Email:** √âquipe YANSNET

---

**Version:** 1.0.0  
**Derni√®re mise √† jour:** Janvier 2025
